"""
RAG ç³»ç»Ÿæ ¸å¿ƒæ¨¡å—
å®ç°åŒå‘é‡åº“æ¶æ„ã€æ–‡æ¡£ç´¢å¼•ã€æ£€ç´¢åŠŸèƒ½
"""

import os
from typing import List, Tuple, Optional
import streamlit as st
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.schema import Document


def loadAndIndexFiles(
    file_paths: List[str],
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    source_type: str = "base",
    additional_metadata: Optional[dict] = None
) -> Tuple[List[Document], int]:
    """
    é€šç”¨æ–‡æ¡£åŠ è½½å’Œç´¢å¼•å‡½æ•°
    
    ç”¨äºåŠ è½½PDFæ–‡ä»¶ã€åˆ†å‰²æ–‡æœ¬å¹¶è¿”å›æ–‡æ¡£ç‰‡æ®µ
    è¿™ä¸ªå‡½æ•°è¢«åŸºç¡€å‘é‡åº“å’Œç”¨æˆ·å‘é‡åº“å…±åŒä½¿ç”¨
    
    Args:
        file_paths: PDF æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        chunk_size: æ–‡æœ¬å—å¤§å°
        chunk_overlap: æ–‡æœ¬å—é‡å å¤§å°
        source_type: æ–‡æ¡£æ¥æºç±»å‹ ("base" æˆ– "user")
        additional_metadata: é¢å¤–çš„å…ƒæ•°æ®ï¼ˆç”¨äºç”¨æˆ·ä¸Šä¼ æ–‡æ¡£ï¼‰
        
    Returns:
        (æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨, åŸå§‹æ–‡æ¡£æ•°é‡)
    """
    all_docs = []
    
    # åŠ è½½æ‰€æœ‰PDFæ–‡ä»¶
    for file_path in file_paths:
        try:
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            
            # æ·»åŠ å…ƒæ•°æ®
            for doc in docs:
                doc.metadata['source_type'] = source_type
                if additional_metadata:
                    doc.metadata.update(additional_metadata)
            
            all_docs.extend(docs)
        except Exception as e:
            st.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
    
    if not all_docs:
        return [], 0
    
    # åˆ†å‰²æ–‡æœ¬
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    splits = text_splitter.split_documents(all_docs)
    
    return splits, len(all_docs)


class DualVectorStoreRAG:
    """åŒå‘é‡åº“ RAG ç³»ç»Ÿ"""
    
    def __init__(
        self,
        base_persist_dir: str = "./chroma_db/base",
        user_persist_dir: str = "./chroma_db/user",
        base_docs_dir: str = "CourseMaterials/deep_learning",
        embedding_model: str = "text-embedding-3-small"
    ):
        """
        åˆå§‹åŒ–åŒå‘é‡åº“ RAG ç³»ç»Ÿ
        
        Args:
            base_persist_dir: åŸºç¡€å‘é‡åº“æŒä¹…åŒ–ç›®å½•
            user_persist_dir: ç”¨æˆ·å‘é‡åº“æŒä¹…åŒ–ç›®å½•
            base_docs_dir: åŸºç¡€æ–‡æ¡£ç›®å½•
            embedding_model: OpenAI embedding æ¨¡å‹åç§°
        """
        self.base_persist_dir = base_persist_dir
        self.user_persist_dir = user_persist_dir
        self.base_docs_dir = base_docs_dir
        self.embedding_model = embedding_model
        
        # åˆ›å»ºç›®å½•
        os.makedirs(base_persist_dir, exist_ok=True)
        os.makedirs(user_persist_dir, exist_ok=True)
        
        # åˆå§‹åŒ– embedding å‡½æ•°
        self.embedding_function = OpenAIEmbeddings(model=embedding_model)
        
        # åˆå§‹åŒ–å‘é‡åº“
        self.base_vectorstore = None
        self.user_vectorstore = None
        self.base_doc_count = 0
        
    def initialize_base_vectorstore(self) -> int:
        """
        åˆå§‹åŒ–æˆ–åŠ è½½åŸºç¡€å‘é‡åº“
        
        Returns:
            åŠ è½½çš„æ–‡æ¡£æ•°é‡
        """
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æŒä¹…åŒ–çš„å‘é‡åº“
        if os.path.exists(self.base_persist_dir) and os.listdir(self.base_persist_dir):
            # åŠ è½½å·²æœ‰çš„å‘é‡åº“
            try:
                self.base_vectorstore = Chroma(
                    persist_directory=self.base_persist_dir,
                    embedding_function=self.embedding_function
                )
                # å°è¯•è·å–æ–‡æ¡£æ•°é‡
                try:
                    collection = self.base_vectorstore._collection
                    self.base_doc_count = collection.count()
                except:
                    self.base_doc_count = 0
                
                return self.base_doc_count
            except Exception as e:
                st.warning(f"âš ï¸ åŠ è½½åŸºç¡€å‘é‡åº“å¤±è´¥ï¼Œå°†é‡æ–°åˆ›å»ºï¼š{str(e)}")
        
        # é¦–æ¬¡åˆ›å»ºï¼šåŠ è½½åŸºç¡€æ–‡æ¡£
        if not os.path.exists(self.base_docs_dir):
            st.error(f"âŒ åŸºç¡€æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨ï¼š{self.base_docs_dir}")
            return 0
        
        # è·å–æ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = []
        for root, dirs, files in os.walk(self.base_docs_dir):
            for file in files:
                if file.endswith('.pdf'):
                    pdf_files.append(os.path.join(root, file))
        
        if not pdf_files:
            st.warning(f"âš ï¸ åœ¨ {self.base_docs_dir} ä¸­æœªæ‰¾åˆ° PDF æ–‡ä»¶")
            return 0
        
        # åŠ è½½å’Œç´¢å¼•æ–‡ä»¶
        splits, doc_count = loadAndIndexFiles(
            file_paths=pdf_files,
            source_type="base"
        )
        
        if not splits:
            st.error("âŒ æœªèƒ½åŠ è½½ä»»ä½•åŸºç¡€æ–‡æ¡£")
            return 0
        
        # åˆ›å»ºå‘é‡åº“
        self.base_vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.embedding_function,
            persist_directory=self.base_persist_dir
        )
        
        self.base_doc_count = doc_count
        return doc_count
    
    def initialize_user_vectorstore(self):
        """åˆå§‹åŒ–æˆ–åŠ è½½ç”¨æˆ·å‘é‡åº“"""
        # å§‹ç»ˆå°è¯•åŠ è½½ç”¨æˆ·å‘é‡åº“ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
        self.user_vectorstore = Chroma(
            persist_directory=self.user_persist_dir,
            embedding_function=self.embedding_function
        )
    
    def add_user_document(
        self,
        file_path: str,
        original_filename: str,
        upload_time: str,
        file_size: int
    ) -> Tuple[bool, str, int]:
        """
        æ·»åŠ ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£åˆ°ç”¨æˆ·å‘é‡åº“
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            original_filename: åŸå§‹æ–‡ä»¶å
            upload_time: ä¸Šä¼ æ—¶é—´
            file_size: æ–‡ä»¶å¤§å°
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, æ¶ˆæ¯, æ·»åŠ çš„æ–‡æœ¬å—æ•°é‡)
        """
        try:
            # åŠ è½½å’Œç´¢å¼•æ–‡ä»¶
            additional_metadata = {
                'original_filename': original_filename,
                'upload_time': upload_time,
                'file_size': file_size
            }
            
            splits, doc_count = loadAndIndexFiles(
                file_paths=[file_path],
                source_type="user",
                additional_metadata=additional_metadata
            )
            
            if not splits:
                return False, "âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ï¼šæœªèƒ½æå–ä»»ä½•å†…å®¹", 0
            
            # æ·»åŠ åˆ°ç”¨æˆ·å‘é‡åº“
            if self.user_vectorstore is None:
                self.initialize_user_vectorstore()
            
            self.user_vectorstore.add_documents(splits)
            
            return True, f"âœ… æˆåŠŸç´¢å¼•æ–‡æ¡£ï¼Œæ·»åŠ äº† {len(splits)} ä¸ªæ–‡æœ¬å—", len(splits)
            
        except Exception as e:
            return False, f"âŒ ç´¢å¼•æ–‡æ¡£æ—¶å‡ºé”™ï¼š{str(e)}", 0
    
    def remove_user_document(self, original_filename: str) -> Tuple[bool, str]:
        """
        ä»ç”¨æˆ·å‘é‡åº“ä¸­åˆ é™¤æ–‡æ¡£
        
        Args:
            original_filename: åŸå§‹æ–‡ä»¶å
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, æ¶ˆæ¯)
        """
        try:
            if self.user_vectorstore is None:
                return True, "ç”¨æˆ·å‘é‡åº“ä¸ºç©º"
            
            # é€šè¿‡å…ƒæ•°æ®è¿‡æ»¤åˆ é™¤
            # æ³¨æ„ï¼šChroma çš„åˆ é™¤æ“ä½œéœ€è¦æ–‡æ¡£IDï¼Œè¿™é‡Œæˆ‘ä»¬éœ€è¦å…ˆæŸ¥è¯¢
            collection = self.user_vectorstore._collection
            results = collection.get(
                where={"original_filename": original_filename}
            )
            
            if results and 'ids' in results and results['ids']:
                collection.delete(ids=results['ids'])
                return True, f"âœ… å·²ä»å‘é‡åº“ä¸­åˆ é™¤ {len(results['ids'])} ä¸ªæ–‡æœ¬å—"
            else:
                return True, "å‘é‡åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
                
        except Exception as e:
            return False, f"âš ï¸ ä»å‘é‡åº“åˆ é™¤æ—¶å‡ºé”™ï¼š{str(e)}"
    
    def create_rag_chain(self, k: int = 3):
        """
        åˆ›å»º RAG æ£€ç´¢é“¾
        
        Args:
            k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            RAG chain
        """
        # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
        def hybrid_retrieve(query: str) -> List[Document]:
            """ä»ä¸¤ä¸ªå‘é‡åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
            all_docs = []
            
            # ä»åŸºç¡€åº“æ£€ç´¢
            if self.base_vectorstore:
                try:
                    base_docs = self.base_vectorstore.similarity_search(query, k=k)
                    all_docs.extend(base_docs)
                except Exception as e:
                    st.warning(f"âš ï¸ åŸºç¡€åº“æ£€ç´¢å¤±è´¥ï¼š{str(e)}")
            
            # ä»ç”¨æˆ·åº“æ£€ç´¢
            if self.user_vectorstore:
                try:
                    # æ£€æŸ¥ç”¨æˆ·åº“æ˜¯å¦æœ‰å†…å®¹
                    collection = self.user_vectorstore._collection
                    if collection.count() > 0:
                        user_docs = self.user_vectorstore.similarity_search(query, k=k)
                        all_docs.extend(user_docs)
                except Exception as e:
                    # ç”¨æˆ·åº“å¯èƒ½ä¸ºç©ºï¼Œè¿™æ˜¯æ­£å¸¸çš„
                    pass
            
            # è¿”å›å‰ k ä¸ªæ–‡æ¡£ï¼ˆå¯ä»¥æ·»åŠ é‡æ–°æ’åºé€»è¾‘ï¼‰
            return all_docs[:k]
        
        # æ ¼å¼åŒ–æ–‡æ¡£ï¼Œæ·»åŠ æ¥æºæ ‡è®°
        def format_docs_with_source(docs: List[Document]) -> str:
            """æ ¼å¼åŒ–æ–‡æ¡£å¹¶æ ‡è®°æ¥æº"""
            parts = []
            for i, d in enumerate(docs, 1):
                src = d.metadata.get("source", "unknown_source")
                src_type = d.metadata.get("source_type", "base")
                page = d.metadata.get("page_label", d.metadata.get("page", "unknown_page"))
                
                # æ ¹æ®æ¥æºç±»å‹é€‰æ‹©å›¾æ ‡
                if src_type == "user":
                    emoji = "ğŸ“„"
                    original_name = d.metadata.get("original_filename", "Unknown")
                    upload_time = d.metadata.get("upload_time", "Unknown")
                    header = f"{emoji} [{i}] ç”¨æˆ·æ–‡æ¡£ï¼š{original_name} (ä¸Šä¼ äº {upload_time}, p.{page})"
                else:
                    emoji = "ğŸ“˜"
                    header = f"{emoji} [{i}] è¯¾ç¨‹ææ–™ï¼š{os.path.basename(src)}, p.{page}"
                
                text = (d.page_content or "").strip()
                parts.append(f"{header}\n{text}")
            
            return "\n\n".join(parts)
        
        # æ„å»º RAG é“¾
        prompt_template = """You are a helpful assistant.
Answer the question using ONLY the Context below.
If the answer is not in the Context, say "I don't know based on the provided context."

Context:
{context}

Question:
{question}
"""
        prompt = ChatPromptTemplate.from_template(prompt_template)
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
        
        rag_chain = (
            {
                "context": RunnableLambda(hybrid_retrieve) | RunnableLambda(format_docs_with_source),
                "question": RunnablePassthrough()
            }
            | prompt
            | llm
        )
        
        return rag_chain

